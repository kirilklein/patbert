{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial for Med-BERT\n",
    "Step-by-step guide to the PyTorch implementation of [Med-BERT](https://www.nature.com/articles/s41746-021-00455-y) \n",
    "\n",
    "<sub><sup><em>\"Med-BERT: pretrained contextualized embeddings on large-scale structured electronic health records for disease prediction.\" NPJ digital medicine 4.1 (2021): 1-13., Rasmy, Laila, et al. <em><sub><sup>\n",
    "\n",
    "------------------------------------------\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The goal of Med-Bert is to obtain good representations of elctronic health records (EHR) to make predicitons for downstream tasks. <br>\n",
    "In order to do so we leverage the power of the pretraining fine-tuning paradigm using a transformer architecture $^{1}$.  \n",
    "Originally used for Natural Language Processing, the transformers have proven their universality by showing SoTA results in fields like computer vision $^2$ and speech recognition $^3$. <br>\n",
    "Recently, a variant of the transformers, called BERT $^{4}$ has also been applied to medical data and electronic health records in particular $^{5-7}$.<br> \n",
    "There are countless tutorials that explain the theory and basic concepts behind the Transformers and BERT as well as their applicaiton to NLP, so here we will focus on using BERT for EHR specifically.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preparation\n",
    "\n",
    "All we need for the input are diagnosis codes and the dates of the visit (hospital or GP visit) in which these codes were assigned.<br>\n",
    "We then construct a nested lists which consists of:\n",
    "1. Patient ID\n",
    "2. The length of stay (LOS) for each visit\n",
    "3. Diagnosis Codes\n",
    "4. Visit number \n",
    "\n",
    "for each patient.\n",
    "\n",
    "#### Example:<br>\n",
    "Assume that patient 0 with id ```'p0'``` has 2 visits with 5 and 20 days length. The first visit has 2 codes ```['M432', 'D321']``` and the second visit has one code ```['S839']```<br>\n",
    "Then the first entry of the data list looks as follows:<br>\n",
    "```['p0', [5, 20], ['M432', 'D321','S839'], [1, 1, 2]]```\n",
    "\n",
    "The BERT model will take inputs of size ```(batch_size, sequence_len, hidden_dim)```.<br>\n",
    "To get the right format we need to first tokenize the data and then get vector embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import pprint \n",
    "pp = pprint.PrettyPrinter()\n",
    "\n",
    "with open('../tutorial/example_data.pkl', 'rb') as f:\n",
    "    example_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Tokenization\n",
    "In this step we simply assign integers to each unique code and bring all sequences to the same length.<br>\n",
    "Additional tokens are needed to Mask inputs and to take care of new codes that are not in the vocab dictionary that might appear in the future.\n",
    "\n",
    "We create a class to take care of tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EHRTokenizer():\n",
    "    # First, we define the vocabulary dict with the special tokens\n",
    "    def __init__(self, vocabulary=None):\n",
    "        if isinstance(vocabulary, type(None)):\n",
    "            self.vocabulary = {\n",
    "                'PAD':0, # padding token\n",
    "                'MASK':1, # masking token (for masked language modeling)\n",
    "                'UNK':2, # unknown token (for out-of-vocabulary tokens)\n",
    "            }\n",
    "            # BERT does not use the 'CLS' and 'SEP' tokens\n",
    "        else:\n",
    "            self.vocabulary = vocabulary\n",
    "    def __call__(self, seq):\n",
    "        return self.batch_encode(seq)\n",
    "\n",
    "    def encode(self, seq):\n",
    "        # create a new token for each new code\n",
    "        for code in seq:\n",
    "            if code not in self.vocabulary:\n",
    "                self.vocabulary[code] = len(self.vocabulary)\n",
    "        return [self.vocabulary[code] for code in seq]\n",
    "\n",
    "    def batch_encode(self, seqs, max_len=None):\n",
    "        # we construct a dictionary to store the tokenized data\n",
    "        if isinstance(max_len, type(None)):\n",
    "            max_len = max([len(seq) for seq in seqs])\n",
    "        pat_ids = [seq[0] for seq in seqs]\n",
    "        los_seqs = [seq[1] for seq in seqs]\n",
    "        code_seqs = [seq[2] for seq in seqs] # icd codes\n",
    "        visit_seqs = [seq[3] for seq in seqs]\n",
    "        if isinstance(max_len, type(None)):\n",
    "            max_len = max([len(seq) for seq in code_seqs])    \n",
    "        output_code_seqs = []\n",
    "        output_visit_seqs = []\n",
    "        for code_seq, visit_seq in zip(code_seqs, visit_seqs):\n",
    "            # truncation\n",
    "            if len(code_seq)>max_len:\n",
    "                code_seq = code_seq[:max_len]\n",
    "                visit_seq = visit_seq[:max_len]\n",
    "            # Tokenizing\n",
    "            tokenized_code_seq = self.encode(code_seq)\n",
    "            output_code_seqs.append(tokenized_code_seq)\n",
    "            output_visit_seqs.append(visit_seq)\n",
    "        tokenized_data_dic = {'pats':pat_ids, 'los':los_seqs, 'codes':output_code_seqs, \n",
    "                            'segments':output_visit_seqs}\n",
    "        return tokenized_data_dic\n",
    "\n",
    "    def save_vocab(self, dest):\n",
    "        # save the vocabulary\n",
    "        print(f\"Writing vocab to {dest}\")\n",
    "        torch.save(self.vocabulary, dest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's run the tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing vocab to ../tutorial/vocab.pt\n",
      "'pats [0, 1, 2]'\n",
      "'los [[1, 24, 18, 20], [27, 12], [22, 1, 18]]'\n",
      "'codes [[3, 4, 5, 6, 7, 8, 9, 10], [11, 12, 13, 14], [15, 16, 17, 18, 19, 20]]'\n",
      "'segments [[1, 1, 2, 2, 3, 3, 4, 4], [1, 1, 1, 2], [1, 1, 2, 2, 2, 3]]'\n",
      "Vocabulary:\n",
      "PAD 0\n",
      "MASK 1\n",
      "UNK 2\n",
      "M29.7 3\n",
      "C49.2 4\n",
      "P12.3 5\n"
     ]
    }
   ],
   "source": [
    "Tokenizer = EHRTokenizer()\n",
    "tokenized_data_dic = Tokenizer.batch_encode(example_data, max_len=20)\n",
    "torch.save(tokenized_data_dic, '../tutorial/tokenized.pt')\n",
    "Tokenizer.save_vocab('../tutorial/vocab.pt')\n",
    "# Lets look at the tokenized data\n",
    "for k,v in tokenized_data_dic.items():\n",
    "    pp.pprint(f\"{k} {v[:3]}\")\n",
    "# Lets look at the vocabulary\n",
    "print('Vocabulary:')\n",
    "for i, (k,v) in enumerate(Tokenizer.vocabulary.items()):\n",
    "    if i>5:\n",
    "        break\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "<sub><sup>[1] Vaswani, Ashish, et al. \"Attention is all you need.\" Advances in neural information processing systems 30 (2017).</sub></sup> <br>\n",
    "<sub><sup>[2] Dosovitskiy, Alexey, et al. \"An image is worth 16x16 words: Transformers for image recognition at scale.\" arXiv preprint arXiv:2010.11929 (2020).</sub></sup> <br>\n",
    "<sub><sup>[3] Polyak, Adam, et al. \"Speech resynthesis from discrete disentangled self-supervised representations.\" arXiv preprint arXiv:2104.00355 (2021).</sub></sup><br>\n",
    "<sub><sup>[4] Devlin, Jacob, et al. \"Bert: Pre-training of deep bidirectional transformers for language understanding.\" arXiv preprint arXiv:1810.04805 (2018).</sub></sup><br>\n",
    "<sub><sup>[5] Li, Yikuan, et al. \"BEHRT: transformer for electronic health records.\" Scientific reports 10.1 (2020): 1-12..</sub></sup><br>\n",
    "<sub><sup>[6] Shang, Junyuan, et al. \"Pre-training of graph augmented transformers for medication recommendation.\" arXiv preprint arXiv:1906.00346 (2019).</sub></sup><br>\n",
    "<sub><sup>[7] Pang, Chao, et al. \"CEHR-BERT: Incorporating temporal information from structured EHR data to improve prediction tasks.\" Machine Learning for Health. PMLR, 2021.</sub></sup><br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('patbert')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4d17b095f833cb6f6ab98a8b16539b4f83b513338040a731dc495c8057944685"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
