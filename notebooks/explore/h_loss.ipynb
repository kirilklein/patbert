{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fjn197\\Miniconda3\\envs\\patbert\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import softmax\n",
    "from torch.nn import CrossEntropyLoss, Softmax\n",
    "from patbert.common import medical\n",
    "from os.path import join\n",
    "data_path = \"C:\\\\Users\\\\fjn197\\\\PhD\\\\projects\\\\PHAIR\\\\pipelines\\\\patbert\\\\data\"\n",
    "import pickle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask rows which equal another tensor\n",
    "def mask_not_equal(tensor, other):\n",
    "    for sample in other:\n",
    "        tensor = tensor[~(tensor == sample).all(dim=1)]\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_parents(children, level):\n",
    "    parent = children.clone()\n",
    "    parent[:,-level] = 0\n",
    "    return parent.unique(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_leaf_nodes(tree):\n",
    "    all_leafs =[]\n",
    "    for i in range(tree.shape[1]-1):\n",
    "        leaf_mask = tree[:,-(i+1)]!=0\n",
    "        leafs = tree[leaf_mask]\n",
    "        tree = tree[~leaf_mask]\n",
    "        all_leafs.append(leafs)\n",
    "        parent = get_unique_parents(leafs, i+1)\n",
    "        tree = mask_not_equal(tree, parent)\n",
    "    all_leafs.append(tree)\n",
    "    return torch.cat(all_leafs, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1],\n",
       "        [1, 1, 2],\n",
       "        [1, 2, 0],\n",
       "        [2, 1, 0],\n",
       "        [2, 2, 0],\n",
       "        [3, 0, 0]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree = torch.tensor([[1, 1, 0], [1,1,1], [1,1,2], [1,2,0],[1,0,0],[2,1,0],[2,2,0],[3,0,0]])\n",
    "select_leaf_nodes(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 2],\n",
      "        [1, 2, 0],\n",
      "        [2, 1, 0],\n",
      "        [2, 2, 0],\n",
      "        [3, 0, 0]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 0],\n",
       "        [2, 1, 0],\n",
       "        [2, 2, 0],\n",
       "        [3, 0, 0]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "leafs_0 = tree[tree[:,-1]!=0]\n",
    "temp = leafs_0\n",
    "temp[:,-1] = 0\n",
    "temp = temp.unique(dim=0)\n",
    "#remove temp from tree\n",
    "tree = mask_not_equal(tree, temp)\n",
    "print(tree)\n",
    "# remove lowest leafs from tree\n",
    "tree = tree[tree[:,-1]==0]\n",
    "leafs_1 = tree[tree[:,-1]!=0]\n",
    "# remove lowest leafs from tree\n",
    "tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_vocab = torch.load(join(data_path,\"tokenized\\\\mimic3\\\\plain\\\\vocabulary.pt\"))\n",
    "atc_codes = medical.MedicalCodes().get_atc()\n",
    "icd_codes = medical.MedicalCodes().get_icd()\n",
    "codes = []\n",
    "codes.append('[PAD]')\n",
    "codes = codes + atc_codes[:5] + icd_codes[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[PAD]',\n",
       " 'MA01',\n",
       " 'MA01A',\n",
       " 'MA01AA',\n",
       " 'MA01AA01',\n",
       " 'MA01AA02',\n",
       " 'DA00',\n",
       " 'DA000',\n",
       " 'DA001',\n",
       " 'DA009',\n",
       " 'DA01']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_vocab = {}\n",
    "for random_code in codes:\n",
    "    main_vocab[random_code] = len(main_vocab)\n",
    "vocab_ls = medical.SKSVocabConstructor(main_vocab)()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 2, 11, 0, 0]\n",
      "[2, 1, 3, 0, 0, 0]\n",
      "[2, 1, 3, 12, 12, 4]\n"
     ]
    }
   ],
   "source": [
    "pad = []\n",
    "ma01 = []\n",
    "ma01aa02  = []\n",
    "da009 = []\n",
    "for vocab in vocab_ls:\n",
    "    pad.append(vocab['[PAD]'])\n",
    "    da009.append(vocab['DA009'])\n",
    "    ma01.append(vocab['MA01'])\n",
    "    ma01aa02.append(vocab['MA01AA02'])\n",
    "print(pad)\n",
    "print(da009)\n",
    "print(ma01)\n",
    "print(ma01aa02)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "2\n",
      "11\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for vocab in vocab_ls:\n",
    "    print(vocab['DA009'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {\n",
    "    'D':1,\n",
    "    'M':2,\n",
    "    'DA00':10,\n",
    "    'DA01':11,\n",
    "    'DB99':13,\n",
    "    'MA00':20,\n",
    "    'DA000':100,\n",
    "    'DA001':101,\n",
    "    'MA000':200,\n",
    "    'MA001':201,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ex_pred(ids):\n",
    "    ex_pred = torch.rand(204)\n",
    "    for id in ids:\n",
    "        ex_pred[id] = 100\n",
    "    return softmax(ex_pred)\n",
    "def get_targets(target_id):\n",
    "    targets_ls = []\n",
    "    while target_id>0:\n",
    "        targets = torch.zeros(204)\n",
    "        targets[target_id] = 1\n",
    "        targets_ls.append(targets)\n",
    "        target_id = target_id//10\n",
    "    return targets_ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cross entropy loss\n",
    "ce = CrossEntropyLoss()\n",
    "def f_loss(pred, targets):\n",
    "    loss = 0 \n",
    "    for i, target in enumerate(targets):\n",
    "        loss+= 1/(10**i)*float(ce(pred.unsqueeze(0), target.unsqueeze(0)) )\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_id = 201\n",
    "targets = get_targets(target_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = torch.zeros(10)\n",
    "target[0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0., 10., 10.,  0.,  5.,  5.])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(logits, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.7835e-44, 1.0000e+00])\n",
      "tensor([1.0000e+00, 3.7835e-44])\n",
      "tensor([3.7835e-44, 1.0000e+00])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "0D or 1D target tensor expected, multi-target not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[39mprint\u001b[39m(level1_probs_A)\n\u001b[0;32m      8\u001b[0m \u001b[39mprint\u001b[39m(level1_probs_B)\n\u001b[1;32m----> 9\u001b[0m CE0 \u001b[39m=\u001b[39m CrossEntropyLoss()(torch\u001b[39m.\u001b[39;49mlog(level0_probs)\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m0\u001b[39;49m), torch\u001b[39m.\u001b[39;49mtensor([\u001b[39m0\u001b[39;49m])\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m0\u001b[39;49m))\n",
      "File \u001b[1;32mc:\\Users\\fjn197\\Miniconda3\\envs\\patbert\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\fjn197\\Miniconda3\\envs\\patbert\\lib\\site-packages\\torch\\nn\\modules\\loss.py:1174\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1173\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m-> 1174\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[0;32m   1175\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[0;32m   1176\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
      "File \u001b[1;32mc:\\Users\\fjn197\\Miniconda3\\envs\\patbert\\lib\\site-packages\\torch\\nn\\functional.py:3026\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3024\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   3025\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3026\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: 0D or 1D target tensor expected, multi-target not supported"
     ]
    }
   ],
   "source": [
    "tokens = ['A', 'B', 'Aa', 'Ab', 'Ba', 'Bb']\n",
    "logits = [0, 100, 100, 0, 0, 100]\n",
    "level0_probs = Softmax(dim=0)(torch.tensor(logits[:2], dtype=torch.float32))\n",
    "level1_probs_A = Softmax(dim=0)(torch.tensor(logits[2:4], dtype=torch.float32))\n",
    "level1_probs_B = Softmax(dim=0)(torch.tensor(logits[4:], dtype=torch.float32))\n",
    "print(level0_probs)\n",
    "print(level1_probs_A)\n",
    "print(level1_probs_B)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "patbert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8fb6e411e82c8a5d6c2a8bdc5fbccfac7dec16b04a57a53901636bf887f1e6c5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
