{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fjn197\\Miniconda3\\envs\\patbert\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import softmax\n",
    "from torch.nn import CrossEntropyLoss, NLLLoss, KLDivLoss\n",
    "from torch import nn\n",
    "from patbert.common import medical\n",
    "from os.path import join\n",
    "data_path = \"C:\\\\Users\\\\fjn197\\\\PhD\\\\projects\\\\PHAIR\\\\pipelines\\\\patbert\\\\data\"\n",
    "import pickle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytorch crossentropyloss takes logits, since we want to operate on probabilities, it's better to use NLL and sum,\n",
    "let's see if we get the same results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CE tensor(0.7023)\n",
      "NLL tensor(0.7023)\n",
      "We can use NLL loss with logprobs\n"
     ]
    }
   ],
   "source": [
    "logits = torch.tensor([1,5,5], dtype=torch.float32)\n",
    "logprobs = torch.log(softmax(logits, dim=0))\n",
    "clas_true = torch.tensor([1])\n",
    "cr = CrossEntropyLoss()\n",
    "print('CE',cr(logits.view(1,-1), clas_true))\n",
    "nll = NLLLoss()\n",
    "print('NLL',nll(logprobs.view(1,-1), clas_true))\n",
    "cr(logits.view(1,-1), clas_true)==nll(logprobs.view(1,-1), clas_true)\n",
    "print(\"We can use NLL loss with logprobs\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we need also a loss which can take soft labels, here we use directly KL divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fjn197\\Miniconda3\\envs\\patbert\\lib\\site-packages\\torch\\nn\\functional.py:2916: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(-0.3333)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kl = KLDivLoss()\n",
    "probs = torch.tensor([[1, 0.8, 0.1],[0.2,0.7,1]], dtype=torch.float)\n",
    "probs_true = torch.tensor([[1, 0, 0],[0,0,1]], dtype=torch.float)\n",
    "kl(probs.view(1,-1), probs_true.view(1,-1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets try to implement the masking with a target in parallel, to access leaf nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.1915e-01, 8.8044e-01, 2.9535e-04, 1.0865e-04, 5.4096e-06],\n",
      "        [2.0199e-01, 5.4908e-01, 1.0031e-01, 7.4310e-02, 7.4310e-02]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# example targets and tree\n",
    "leaf_nodes = torch.tensor([[1,2,1],[1,2,2], [1,1,0], [2,1,2],[2,1,3]])\n",
    "y_true_enc = torch.tensor([[[1,2,0], [1,0,0], [2,1,0], [2,1,0]],\n",
    "                        [[1,2,0], [1,1,0], [2,0,0], [2,1,0],]], requires_grad=False)\n",
    "leaf_logits = torch.tensor([[10.0, 12, 4, 3, 0],[1,2,.3,0,0]], dtype=torch.float32, requires_grad=True)\n",
    "leaf_probs = softmax(leaf_logits, dim=-1)\n",
    "print(leaf_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should be part of a class, leaf nodes stay the same\n",
    "def get_leaf_node_probabilities(leaf_probs:torch.tensor, y_true_enc: torch.tensor, leaf_nodes: torch.tensor):\n",
    "    \"\"\"Selects leaf probabilities for a given target tensor.\n",
    "    Args:\n",
    "        leaf_probs (torch.tensor): Probabilities (batchsize, num_leaf_nodes)\n",
    "        y_true_enc (torch.tensor): Target vector (batchsize, seq_len, levels)\n",
    "        leaf_nodes (torch.tensor): Leaf nodes (num_leaf_nodes, levels)\n",
    "    Returns:\n",
    "        torch.tensor: Selected leaf probabilities (batchsize, seq_len)\"\"\"\n",
    "    # we want to match all the leaf nodes with a target, e.g. target: 1,2,0 should select 1,2,1 and 1,2,2\n",
    "    zeros_mask = y_true_enc == 0\n",
    "    leaf_mask = (leaf_nodes == y_true_enc[:, :, None, :]) | zeros_mask[:, :, None,:] # select all leaf nodes that match the target\n",
    "    leaf_mask = leaf_mask.all(dim=-1).to(torch.int16)\n",
    "\n",
    "    leaf_probs = leaf_probs[:,None,:].expand(leaf_mask.shape) # batch, seq_len, num_leafes\n",
    "    selected_leaf_probs = leaf_probs * leaf_mask\n",
    "    selected_leaf_probs = selected_leaf_probs.sum(dim=-1)\n",
    "    return selected_leaf_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.0696, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final_probs = get_leaf_node_probabilities(leaf_logits, y_true_enc, leaf_nodes)\n",
    "def flat_softmax_cross_entropy(leaf_logits, y_true_enc, leaf_nodes):\n",
    "    leaf_probs = softmax(leaf_logits, dim=-1)\n",
    "    selected_leaf_probs = get_leaf_node_probabilities(leaf_probs, y_true_enc, leaf_nodes)\n",
    "    # print(selected_leaf_probs)\n",
    "    log_probs = torch.log(selected_leaf_probs)\n",
    "    log_probs = log_probs.flatten() # batchsize * seq_len\n",
    "    loss = nll(log_probs.unsqueeze(-1), torch.zeros_like(log_probs, dtype=torch.int64))\n",
    "    return loss\n",
    "flat_softmax_cross_entropy(leaf_logits, y_true_enc, leaf_nodes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We can alternatively perform matrix multiplication to go up one level of hierarchy from the leaf probabilities\n",
    " for example if we have three leaf probabilities [p1, p2, p3] and p1 and p2 are from the same parent, we would multiply by a matrix [[1, 1, 0], [0,0,1]] from the left, to get [p1+p2, p3]\n",
    " for details see Hierarchical Classification at multiple operating points"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss on multiple levels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it might be easier to construct the targets on different levels and compute the loss as we did previously\n",
    "e.g. [1,2,0] would be [1,0,0], [1,2,0], [1,2,int] on the three levels that are present\n",
    "We might either say that on the lowest level we don't have a target and the loss is thus 0*log(q_i) or we maximize the entropy on levels below to not allow for random predictions, meaning\n",
    "l_below_target = 1/n*sum(log(q_i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.1912e-01, 8.8018e-01, 2.9527e-04, 1.0862e-04, 2.9527e-04, 5.4080e-06],\n",
      "        [1.8802e-01, 5.1110e-01, 9.3369e-02, 6.9170e-02, 6.9170e-02, 6.9170e-02]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# we reuse this example\n",
    "leaf_nodes = torch.tensor([[1,2,1],[1,2,2], [1,1,0], [2,1,2],[2,1,3], [3,0,0]])\n",
    "y_true_enc = torch.tensor([[[1,2,0], [1,0,0], [2,1,0], [2,1,0], [3,0,0]],\n",
    "                        [[1,2,0], [1,1,0], [2,0,0], [2,1,0],[1,0,0]]], requires_grad=False)\n",
    "leaf_logits = torch.tensor([[10.0, 12, 4, 3, 4, 0],[1,2,.3,0,0,0]], dtype=torch.float32, requires_grad=True)\n",
    "leaf_probs = softmax(leaf_logits, dim=-1)\n",
    "print(leaf_probs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Level 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "from patbert.features import utils\n",
    "class FlatSoftmaxMultOP:\n",
    "    def __init__(self, leaf_nodes, trainable_weights=0) -> None:\n",
    "        self.leaf_nodes = leaf_nodes\n",
    "        self.lvl_mappings = self.get_level_mappings()\n",
    "        self.lvl_sel_mats = self.get_level_selection_mats()\n",
    "        weights = self.initialize_geometric_weights()\n",
    "        if trainable_weights:\n",
    "            weights.requires_grad = True\n",
    "        \n",
    "\n",
    "    def __call__(self, predicted_probs:torch.tensor, y_true_enc:torch.tensor)->float:\n",
    "        level_probs = []\n",
    "        for i, mat in enumerate(self.lvl_sel_mats):\n",
    "            print('mat', mat)\n",
    "            print('pred', predicted_probs)\n",
    "            prob_lvl = mat @ predicted_probs\n",
    "            y_true_lvl = y_true_enc[:, :i+1]\n",
    "            print('lvl_prob:', y_true_lvl)\n",
    "\n",
    "\n",
    "    def initialize_geometric_weights(self):\n",
    "        \"\"\"We initialize weights as e**(-i)\"\"\"\n",
    "        return torch.exp(-1*torch.arange(len(self.lvl_sel_mats)))\n",
    "\n",
    "    def get_true_prob(level, y_true_enc):\n",
    "        \"\"\"Returns the probability for a given level and target\"\"\"\n",
    "        pass\n",
    "\n",
    "    def get_level_mappings(self):\n",
    "        \"\"\"Returns a dictionary, where the key is the level and the value is a tensor\n",
    "        with the corresponding indices for this level.\n",
    "        \"\"\"\n",
    "        lvl_mappings = []\n",
    "        for i in range(len(self.leaf_nodes[0])):\n",
    "            leaf_nodes_part = self.leaf_nodes[:,:i+1]\n",
    "            _, unique_indices = torch.unique(leaf_nodes_part, dim=0, return_inverse=True) # we can use these to enumerate the target, in order to access the correct prob\n",
    "            lvl_mappings.append([leaf_nodes_part, unique_indices])\n",
    "        return lvl_mappings\n",
    "\n",
    "    def get_level_selection_mats(self)->List[torch.tensor]:\n",
    "        \"\"\"For every level builds a matrix, such that when multiplied from the left\n",
    "        with leaf probabilities, returns probabs for every node on this level. \n",
    "        Matrices are returned as list. \n",
    "        \"\"\"\n",
    "        mats = []\n",
    "        for i in range(len(self.leaf_nodes[0])):\n",
    "            leaf_nodes_part = self.leaf_nodes[:,:i+1]\n",
    "            # remove zero nodes (below leaf)\n",
    "            unique_nodes, unique_indices = torch.unique(leaf_nodes_part, dim=0, return_inverse=True) # we can use these to enumerate the target, in order to access the correct prob\n",
    "            # print(unique_nodes)\n",
    "            # print('indices:', unique_indices)\n",
    "            # Map each unique row to an integer based on its position in the sorted unique tensor\n",
    "            mat = self.create_leaf_selection_matrix(unique_indices)\n",
    "            # zero means we are below the leaf, so we set the corresponding row to zero\n",
    "            zero_mask = leaf_nodes_part[:,-1]==0\n",
    "            mat[:,zero_mask] = 0 \n",
    "            mats.append(mat)\n",
    "        return mats\n",
    "    # write a function that takes torch.tensor([1,1,1,2,2]) and returns a matrix that looks like this torch.tensor([[1,1,1,0,0],[0,0,0,1,1]])\n",
    "    @staticmethod\n",
    "    def create_leaf_selection_matrix(indices):\n",
    "        \"\"\"This function takes an array of integers and \n",
    "        returns a matrix where each row is a one-hot encoded version of the input.\n",
    "        e,g, [1,1,1,2,2] -> [[1,1,1,0,0],[0,0,0,1,1]]]\"\"\"\n",
    "        unique_values = torch.unique(indices)\n",
    "        mask = (indices.unsqueeze(0) == unique_values.unsqueeze(1)).float()\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 2],\n",
       "         [1, 0],\n",
       "         [2, 1],\n",
       "         [2, 1],\n",
       "         [3, 0]],\n",
       "\n",
       "        [[1, 2],\n",
       "         [1, 1],\n",
       "         [2, 0],\n",
       "         [2, 1],\n",
       "         [1, 0]]])"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_true_prob(level, y_true_enc):\n",
    "    \"\"\"Returns the probability for a given level and target\"\"\"\n",
    "    y_true_enc = y_true_enc[:, :, :level+1]\n",
    "    \n",
    "    return y_true_enc\n",
    "get_true_prob(1, y_true_enc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: figure this part out!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "The shape of the mask [2, 3] at index 1 does not match the shape of the indexed tensor [2, 2, 3] at index 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[270], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m level1_node_ints \u001b[39m=\u001b[39m mappings[\u001b[39m1\u001b[39m][\u001b[39m1\u001b[39m]\n\u001b[0;32m      6\u001b[0m y_enc2 \u001b[39m=\u001b[39m y_true_enc[:,:\u001b[39m2\u001b[39m]\n\u001b[1;32m----> 7\u001b[0m y_enc2 \u001b[39m=\u001b[39m y_enc2[\u001b[39m~\u001b[39;49m(y_enc2\u001b[39m==\u001b[39;49m\u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49many(dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m),:]\n\u001b[0;32m      8\u001b[0m \u001b[39mprint\u001b[39m(y_enc2)\n\u001b[0;32m      9\u001b[0m mask \u001b[39m=\u001b[39m (level1_nodes \u001b[39m==\u001b[39m y_enc2[:,\u001b[39mNone\u001b[39;00m,:])\u001b[39m.\u001b[39mall(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: The shape of the mask [2, 3] at index 1 does not match the shape of the indexed tensor [2, 2, 3] at index 1"
     ]
    }
   ],
   "source": [
    "flat_softmax = FlatSoftmaxMultOP(leaf_nodes, trainable_weights=0)\n",
    "mappings = flat_softmax.lvl_mappings\n",
    "# level 1\n",
    "level1_nodes = mappings[1][0]\n",
    "level1_node_ints = mappings[1][1]\n",
    "y_enc2 = y_true_enc[:,:2]\n",
    "y_enc2 = y_enc2[~(y_enc2==0).any(dim=1),:]\n",
    "print(y_enc2)\n",
    "mask = (level1_nodes == y_enc2[:,None,:]).all(dim=-1)\n",
    "print(mask.shape)\n",
    "print(mask)\n",
    "\n",
    "print(level1_nodes)\n",
    "print(y_true_enc[0][:, :1])\n",
    "# level1_node_ints.expand()[mask]\n",
    "print(level1_node_ints)\n",
    "print(level1_node_ints.expand(4,-1).transpose(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_nodes, _ = torch.unique(leaf_nodes[:,:2], return_inverse=True, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 2],\n",
       "         [1, 0],\n",
       "         [2, 1],\n",
       "         [2, 1],\n",
       "         [3, 0]],\n",
       "\n",
       "        [[1, 2],\n",
       "         [1, 1],\n",
       "         [2, 0],\n",
       "         [2, 1],\n",
       "         [1, 0]]])"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y2 = y_true_enc[:,:,:2]\n",
    "y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1],\n",
      "        [1, 2],\n",
      "        [2, 1],\n",
      "        [3, 0]])\n"
     ]
    }
   ],
   "source": [
    "print(unique_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 2],\n",
       "         [1, 0],\n",
       "         [2, 1],\n",
       "         [2, 1],\n",
       "         [3, 0]],\n",
       "\n",
       "        [[1, 2],\n",
       "         [1, 1],\n",
       "         [2, 0],\n",
       "         [2, 1],\n",
       "         [1, 0]]])"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true_enc[:,:,:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [2, 1],\n",
      "        [2, 1],\n",
      "        [1, 2],\n",
      "        [1, 1],\n",
      "        [2, 1]])\n"
     ]
    }
   ],
   "source": [
    "y_enc2 = y_true_enc[:,:,:2]\n",
    "y_enc2 = y_enc2[~(y_enc2==0).any(dim=2),:]\n",
    "print(y_enc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[268], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m mask \u001b[39m=\u001b[39m (unique_nodes \u001b[39m==\u001b[39m y_enc2[:,:,\u001b[39mNone\u001b[39;49;00m,:])\u001b[39m.\u001b[39mall(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[39mprint\u001b[39m(mask\u001b[39m.\u001b[39mshape)\n\u001b[0;32m      3\u001b[0m \u001b[39mprint\u001b[39m(mask)\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "mask = (unique_nodes == y_enc2[:,:,None,:]).all(dim=-1)\n",
    "print(mask.shape)\n",
    "print(mask)\n",
    "torch.nonzero(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a function that takes torch.tensor([1,1,1,2,2]) and returns a matrix that looks like this torch.tensor([[1,1,1,0,0],[0,0,0,1,1]])\n",
    "def create_leaf_selection_matrix(tensor):\n",
    "    \"\"\"This function takes an array of integers and \n",
    "    returns a matrix where each row is a one-hot encoded version of the input.\n",
    "    e,g, [1,1,1,2,2] -> [[1,1,1,0,0],[0,0,0,1,1]]]\"\"\"\n",
    "    unique_values = torch.unique(tensor)\n",
    "    mask = (tensor.unsqueeze(0) == unique_values.unsqueeze(1)).float()\n",
    "    return mask\n",
    "\n",
    "def get_probs_on_level(probs, leaf_nodes, level):\n",
    "    tensor = leaf_nodes[:, :level]\n",
    "    selection_matrix = create_leaf_selection_matrix(tensor)\n",
    "    return selection_matrix@probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9.9989e-01, 1.1406e-04], grad_fn=<MvBackward0>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_probs_on_level(leaf_probs[0], leaf_nodes, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " the model should return logits for every leaf node:\n",
    " in this case, we will get 6 logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'newleafs' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[157], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39m# this is one of the leaf nodes, so we can use the corresponding probability:\u001b[39;00m\n\u001b[0;32m      4\u001b[0m target \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m])\n\u001b[1;32m----> 5\u001b[0m loss \u001b[39m=\u001b[39m compute_loss(logits, target, leafs)\n\u001b[0;32m      6\u001b[0m target2 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m])\n\u001b[0;32m      7\u001b[0m \u001b[39m# if last entry is zero, we sum over leafs that start with the same integers\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[155], line 8\u001b[0m, in \u001b[0;36mcompute_loss\u001b[1;34m(logits, target, leafs)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m      7\u001b[0m     probs \u001b[39m=\u001b[39m softmax(logits, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m     probs \u001b[39m=\u001b[39m probs[(newleafs \u001b[39m==\u001b[39m target)\u001b[39m.\u001b[39mall(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)]\u001b[39m.\u001b[39msum()\n\u001b[0;32m      9\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m-\u001b[39mtorch\u001b[39m.\u001b[39mlog(probs)\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'newleafs' referenced before assignment"
     ]
    }
   ],
   "source": [
    "logits = torch.tensor([1, 1, 1,1,1,1], dtype=torch.float32)\n",
    "\n",
    "# this is one of the leaf nodes, so we can use the corresponding probability:\n",
    "target = torch.tensor([1, 1, 1])\n",
    "loss = compute_loss(logits, target, leafs)\n",
    "target2 = torch.tensor([1, 1, 0])\n",
    "# if last entry is zero, we sum over leafs that start with the same integers\n",
    "\n",
    "loss2 = compute_loss(logits, target2, leafs)\n",
    "print(loss, loss2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_vocab = torch.load(join(data_path,\"tokenized\\\\mimic3\\\\plain\\\\vocabulary.pt\"))\n",
    "atc_codes = medical.MedicalCodes().get_atc()\n",
    "icd_codes = medical.MedicalCodes().get_icd()\n",
    "codes = []\n",
    "codes.append('[PAD]')\n",
    "codes = codes + atc_codes[:5] + icd_codes[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[PAD]',\n",
       " 'MA01',\n",
       " 'MA01A',\n",
       " 'MA01AA',\n",
       " 'MA01AA01',\n",
       " 'MA01AA02',\n",
       " 'DA00',\n",
       " 'DA000',\n",
       " 'DA001',\n",
       " 'DA009',\n",
       " 'DA01']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_vocab = {}\n",
    "for random_code in codes:\n",
    "    main_vocab[random_code] = len(main_vocab)\n",
    "vocab_ls = medical.SKSVocabConstructor(main_vocab)()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 2, 11, 0, 0]\n",
      "[2, 1, 3, 0, 0, 0]\n",
      "[2, 1, 3, 12, 12, 4]\n"
     ]
    }
   ],
   "source": [
    "pad = []\n",
    "ma01 = []\n",
    "ma01aa02  = []\n",
    "da009 = []\n",
    "for vocab in vocab_ls:\n",
    "    pad.append(vocab['[PAD]'])\n",
    "    da009.append(vocab['DA009'])\n",
    "    ma01.append(vocab['MA01'])\n",
    "    ma01aa02.append(vocab['MA01AA02'])\n",
    "print(pad)\n",
    "print(da009)\n",
    "print(ma01)\n",
    "print(ma01aa02)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "2\n",
      "11\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for vocab in vocab_ls:\n",
    "    print(vocab['DA009'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {\n",
    "    'D':1,\n",
    "    'M':2,\n",
    "    'DA00':10,\n",
    "    'DA01':11,\n",
    "    'DB99':13,\n",
    "    'MA00':20,\n",
    "    'DA000':100,\n",
    "    'DA001':101,\n",
    "    'MA000':200,\n",
    "    'MA001':201,}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ex_pred(ids):\n",
    "    ex_pred = torch.rand(204)\n",
    "    for id in ids:\n",
    "        ex_pred[id] = 100\n",
    "    return softmax(ex_pred)\n",
    "def get_targets(target_id):\n",
    "    targets_ls = []\n",
    "    while target_id>0:\n",
    "        targets = torch.zeros(204)\n",
    "        targets[target_id] = 1\n",
    "        targets_ls.append(targets)\n",
    "        target_id = target_id//10\n",
    "    return targets_ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cross entropy loss\n",
    "ce = CrossEntropyLoss()\n",
    "def f_loss(pred, targets):\n",
    "    loss = 0 \n",
    "    for i, target in enumerate(targets):\n",
    "        loss+= 1/(10**i)*float(ce(pred.unsqueeze(0), target.unsqueeze(0)) )\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_id = 201\n",
    "targets = get_targets(target_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = torch.zeros(10)\n",
    "target[0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0., 10., 10.,  0.,  5.,  5.])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(logits, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.7835e-44, 1.0000e+00])\n",
      "tensor([1.0000e+00, 3.7835e-44])\n",
      "tensor([3.7835e-44, 1.0000e+00])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "0D or 1D target tensor expected, multi-target not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[39mprint\u001b[39m(level1_probs_A)\n\u001b[0;32m      8\u001b[0m \u001b[39mprint\u001b[39m(level1_probs_B)\n\u001b[1;32m----> 9\u001b[0m CE0 \u001b[39m=\u001b[39m CrossEntropyLoss()(torch\u001b[39m.\u001b[39;49mlog(level0_probs)\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m0\u001b[39;49m), torch\u001b[39m.\u001b[39;49mtensor([\u001b[39m0\u001b[39;49m])\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m0\u001b[39;49m))\n",
      "File \u001b[1;32mc:\\Users\\fjn197\\Miniconda3\\envs\\patbert\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\fjn197\\Miniconda3\\envs\\patbert\\lib\\site-packages\\torch\\nn\\modules\\loss.py:1174\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1173\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m-> 1174\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[0;32m   1175\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[0;32m   1176\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
      "File \u001b[1;32mc:\\Users\\fjn197\\Miniconda3\\envs\\patbert\\lib\\site-packages\\torch\\nn\\functional.py:3026\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3024\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   3025\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3026\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: 0D or 1D target tensor expected, multi-target not supported"
     ]
    }
   ],
   "source": [
    "tokens = ['A', 'B', 'Aa', 'Ab', 'Ba', 'Bb']\n",
    "logits = [0, 100, 100, 0, 0, 100]\n",
    "level0_probs = Softmax(dim=0)(torch.tensor(logits[:2], dtype=torch.float32))\n",
    "level1_probs_A = Softmax(dim=0)(torch.tensor(logits[2:4], dtype=torch.float32))\n",
    "level1_probs_B = Softmax(dim=0)(torch.tensor(logits[4:], dtype=torch.float32))\n",
    "print(level0_probs)\n",
    "print(level1_probs_A)\n",
    "print(level1_probs_B)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "patbert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8fb6e411e82c8a5d6c2a8bdc5fbccfac7dec16b04a57a53901636bf887f1e6c5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
